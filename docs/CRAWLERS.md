# Crawlers Guide

> NEUN Web3 Jobs Platform í¬ë¡¤ëŸ¬ ëª©ë¡ ë° ì‚¬ìš© ê°€ì´ë“œ

## Overview

17ê°œ ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ Web3/ë¸”ë¡ì²´ì¸ ê´€ë ¨ ì±„ìš© ê³µê³ ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

**ì‹¤í–‰ ë°©ë²•:**
```bash
npm run crawl
```

**ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰:**
```bash
npm run schedule        # í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬
npm run dev:all         # ê°œë°œ ì„œë²„ + ìŠ¤ì¼€ì¤„ëŸ¬ ë™ì‹œ ì‹¤í–‰
```

---

## Crawler List

### Tier 1: Priority Sources (ë†’ì€ ìš°ì„ ìˆœìœ„)

| Crawler | Source | Region | Description |
|---------|--------|--------|-------------|
| `priority-companies` | ë‹¤ì¤‘ | Global | VC í¬íŠ¸í´ë¦¬ì˜¤ íšŒì‚¬ ì§ì ‘ í¬ë¡¤ë§ |
| `web3career` | web3.career | Global | ìµœëŒ€ Web3 ì±„ìš© í”Œë«í¼ |
| `web3krjobs` | web3kr.jobs | Korea | í•œêµ­ Web3 ì „ë¬¸ ì±„ìš© |
| `cryptojobslist` | cryptojobslist.com | Global | í¬ë¦½í†  ì „ë¬¸ ì±„ìš© ì‚¬ì´íŠ¸ |
| `cryptocurrencyjobs` | cryptocurrencyjobs.co | Global | 700+ í¬ë¦½í†  ì±„ìš© ê³µê³  |

### Tier 2: General Web3 (ì¼ë°˜ Web3)

| Crawler | Source | Region | Description |
|---------|--------|--------|-------------|
| `remote3` | remote3.co | Global | ë¦¬ëª¨íŠ¸ Web3 ì±„ìš© ì „ë¬¸ |
| `remoteok` | remoteok.com | Global | ë¦¬ëª¨íŠ¸ ì „ë¬¸ (Web3 í•„í„°) |
| `rocketpunch` | rocketpunch.com | Korea | í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì±„ìš© (ë¸”ë¡ì²´ì¸ í•„í„°) |
| `cryptojobs` | crypto.jobs | Global | Web3 ì „ë¬¸ ì±„ìš© (3500+ ê³µê³ ) |
| `wellfound` | wellfound.com | Global | ìŠ¤íƒ€íŠ¸ì—… ì±„ìš© (êµ¬ AngelList) |
| `superteam` | talent.superteam.fun | Global | Solana ìƒíƒœê³„ ë°”ìš´í‹°/ì±„ìš© |

### Tier 3: Ecosystem Jobs (ìƒíƒœê³„ë³„)

| Crawler | Source | Region | Description |
|---------|--------|--------|-------------|
| `suijobs` | jobs.sui.io | Global | Sui Foundation ê³µì‹ |
| `solanajobs` | jobs.solana.com | Global | Solana Foundation ê³µì‹ |
| `ethereum` | ethereum.foundation | Global | Ethereum Foundation ê³µì‹ |
| `avalanchejobs` | jobs.avax.network | Global | Avalanche Foundation ê³µì‹ |
| `arbitrumjobs` | jobs.arbitrum.io | Global | Arbitrum Foundation ê³µì‹ |
| `basehirechain` | base.hirechain.io | Global | Base (L2) ìƒíƒœê³„ ì±„ìš© |

### ATS Platform Crawlers (ì±„ìš© í”Œë«í¼)

`scripts/crawlers/platforms/` ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜

| Crawler | Platform | Description |
|---------|----------|-------------|
| `getro` | Getro | VC í¬íŠ¸í´ë¦¬ì˜¤ ì±„ìš© í”Œë«í¼ |
| `greenhouse` | Greenhouse | ê¸°ì—…ìš© ATS |
| `lever` | Lever | ê¸°ì—…ìš© ATS |
| `ashby` | Ashby | ê¸°ì—…ìš© ATS |

---

## Crawler Architecture

```
scripts/
â”œâ”€â”€ crawl.ts                    # ë©”ì¸ ì§„ì…ì 
â”œâ”€â”€ scheduler.ts                # í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬
â”œâ”€â”€ utils.ts                    # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ web3career.ts           # web3.career í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ web3krjobs.ts           # web3kr.jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ cryptojobslist.ts       # cryptojobslist í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ remote3.ts              # remote3.co í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ remoteok.ts             # remoteok.com í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ rocketpunch.ts          # rocketpunch.com í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ suijobs.ts              # Sui jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ solanajobs.ts           # Solana jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ ethereum.ts             # Ethereum Foundation í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ cryptocurrencyjobs.ts   # cryptocurrencyjobs.co í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ cryptojobs.ts           # crypto.jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ wellfound.ts            # wellfound.com í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ superteam.ts            # Superteam Earn í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ basehirechain.ts        # Base Hirechain í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ avalanchejobs.ts        # Avalanche jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ arbitrumjobs.ts         # Arbitrum jobs í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ priority-companies.ts   # VC í¬íŠ¸í´ë¦¬ì˜¤ íšŒì‚¬ í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ platforms/
â”‚   â”‚   â”œâ”€â”€ index.ts            # í”Œë«í¼ í¬ë¡¤ëŸ¬ ì¸ë±ìŠ¤
â”‚   â”‚   â”œâ”€â”€ getro.ts            # Getro ATS í¬ë¡¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ greenhouse.ts       # Greenhouse ATS í¬ë¡¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ lever.ts            # Lever ATS í¬ë¡¤ëŸ¬
â”‚   â”‚   â””â”€â”€ ashby.ts            # Ashby ATS í¬ë¡¤ëŸ¬
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ ...                 # í¬ë¡¤ëŸ¬ ìœ í‹¸ë¦¬í‹°
â””â”€â”€ utils/
    â””â”€â”€ ...                     # ê³µí†µ ìœ í‹¸ë¦¬í‹°
```

---

## How Crawlers Work

### 1. Basic Flow

```
1. fetchHTML(url)          â†’ HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
2. parse($)                â†’ Cheerioë¡œ ë°ì´í„° ì¶”ì¶œ
3. fetchJobDetails(url)    â†’ ìƒì„¸ í˜ì´ì§€ ì¶”ê°€ í¬ë¡¤ë§
4. validateAndSaveJob()    â†’ ìœ íš¨ì„± ê²€ì‚¬ í›„ ì €ì¥
5. logCrawlResult()        â†’ í¬ë¡¤ë§ ê²°ê³¼ ë¡œê¹…
```

### 2. Return Type

ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” ë‹¤ìŒ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

```typescript
interface CrawlerReturn {
  total: number   // ì „ì²´ ì²˜ë¦¬ ê±´ìˆ˜
  new: number     // ì‹ ê·œ ì¶”ê°€ ê±´ìˆ˜
}
```

### 3. Rate Limiting

ê° í¬ë¡¤ëŸ¬ëŠ” Rate Limitì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤:

```typescript
await delay(500)   // í˜ì´ì§€ ê°„ 500ms ë”œë ˆì´
await delay(300)   // ìƒì„¸ í˜ì´ì§€ ê°„ 300ms ë”œë ˆì´
```

---

## Creating a New Crawler

### Template

```typescript
// scripts/crawlers/newsite.ts
import { supabase } from '../../lib/supabase-script'
import { validateAndSaveJob } from '../../lib/validations/validate-job'
import { fetchHTML, delay, cleanText } from '../utils'

interface CrawlerReturn {
  total: number
  new: number
}

export async function crawlNewSite(): Promise<CrawlerReturn> {
  console.log('ğŸš€ Starting NewSite crawler...')

  const baseUrl = 'https://newsite.com/jobs'
  const $ = await fetchHTML(baseUrl)

  if (!$) {
    console.error('âŒ Failed to fetch NewSite')
    return { total: 0, new: 0 }
  }

  const jobs: any[] = []

  // Parse job listings
  $('.job-item').each((_, el) => {
    const title = cleanText($(el).find('.title').text())
    const company = cleanText($(el).find('.company').text())
    const url = $(el).find('a').attr('href')

    if (title && url) {
      jobs.push({ title, company, url })
    }
  })

  console.log(`ğŸ“¦ Found ${jobs.length} jobs`)

  let savedCount = 0
  let newCount = 0

  for (const job of jobs) {
    const result = await validateAndSaveJob({
      title: job.title,
      company: job.company,
      url: job.url,
      location: 'Remote',
      type: 'Full-time',
      category: 'Engineering',
      source: 'newsite',
      region: 'Global',
    }, 'newsite')

    if (result.saved) savedCount++
    if (result.isNew) newCount++

    await delay(100)
  }

  // Log crawl result
  await supabase.from('CrawlLog').insert({
    source: 'newsite',
    status: 'success',
    jobCount: savedCount,
  })

  console.log(`âœ… Saved ${savedCount} jobs (${newCount} new)`)
  return { total: savedCount, new: newCount }
}
```

### Register in Main Crawler

```typescript
// scripts/crawl.ts
import { crawlNewSite } from './crawlers/newsite'

const crawlers = [
  // ... existing crawlers
  { name: 'newsite.com', fn: crawlNewSite },
]
```

---

## Utility Functions

### `scripts/utils.ts`

| Function | Description |
|----------|-------------|
| `fetchHTML(url)` | URLì—ì„œ HTML ê°€ì ¸ì™€ Cheerio ê°ì²´ ë°˜í™˜ |
| `delay(ms)` | ì§€ì • ì‹œê°„ ëŒ€ê¸° |
| `cleanText(text)` | ê³µë°±/ê°œí–‰ ì •ë¦¬ |
| `extractHTML(el, $)` | ìš”ì†Œì˜ HTML ì¶”ì¶œ |
| `parseSalary(text)` | ì—°ë´‰ ë¬¸ìì—´ íŒŒì‹± â†’ { min, max, currency } |
| `detectExperienceLevel(text)` | ê²½ë ¥ ë ˆë²¨ ê°ì§€ |
| `detectRemoteType(text)` | ê·¼ë¬´ í˜•íƒœ ê°ì§€ |

### `lib/validations/validate-job.ts`

```typescript
interface ValidateResult {
  saved: boolean
  isNew: boolean
  error?: string
}

validateAndSaveJob(jobData, source): Promise<ValidateResult>
```

---

## Discord Notifications

í¬ë¡¤ë§ ì‹œì‘/ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ Discord ì›¹í›…ìœ¼ë¡œ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.

**í™˜ê²½ë³€ìˆ˜ ì„¤ì •:**
```bash
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...
```

**ì•Œë¦¼ ë‚´ìš©:**
- ğŸš€ í¬ë¡¤ë§ ì‹œì‘
- âœ… í¬ë¡¤ë§ ì™„ë£Œ (ìƒˆ ê³µê³  ìˆ˜, ì „ì²´ ì²˜ë¦¬ ìˆ˜)
- âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ (ì—ëŸ¬ ë©”ì‹œì§€)

---

## Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| `Failed to fetch` | ì‚¬ì´íŠ¸ ì ‘ê·¼ ë¶ˆê°€ | í”„ë¡ì‹œ ì‚¬ìš© ë˜ëŠ” User-Agent ë³€ê²½ |
| `Rate limited` | ìš”ì²­ ë„ˆë¬´ ë¹ ë¦„ | `delay()` ì‹œê°„ ì¦ê°€ |
| `Empty results` | HTML êµ¬ì¡° ë³€ê²½ | ì…€ë ‰í„° ì—…ë°ì´íŠ¸ |
| `Duplicate jobs` | URL ì¤‘ë³µ | URL ì •ê·œí™” í™•ì¸ |

### Debug Mode

```bash
# íŠ¹ì • í¬ë¡¤ëŸ¬ë§Œ í…ŒìŠ¤íŠ¸
npx tsx scripts/crawlers/web3career.ts

# ë“œë¼ì´ëŸ° (ì €ì¥í•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸)
npx tsx scripts/test-crawler.ts --source=web3career --dry-run
```

---

## Cron Schedule

### Vercel Cron

`vercel.json`:
```json
{
  "crons": [
    {
      "path": "/api/cron/crawl",
      "schedule": "0 */6 * * *"
    }
  ]
}
```

### GitHub Actions

`.github/workflows/crawl.yml`:
```yaml
on:
  schedule:
    - cron: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤
  workflow_dispatch:        # ìˆ˜ë™ ì‹¤í–‰

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm install
      - run: npm run crawl
```

### Local Scheduler

```bash
npm run schedule
```

`scripts/scheduler.ts`ì—ì„œ node-cron ì‚¬ìš©:
```typescript
cron.schedule('0 */6 * * *', async () => {
  await runAllCrawlers()
})
```

---

## Performance Tips

1. **ë³‘ë ¬ ì²˜ë¦¬ ì§€ì–‘**: Rate limit ë•Œë¬¸ì— ìˆœì°¨ ì²˜ë¦¬ ê¶Œì¥
2. **ìƒì„¸ í˜ì´ì§€ ìµœì†Œí™”**: í•„ìˆ˜ ì •ë³´ë§Œ ì¶”ê°€ í¬ë¡¤ë§
3. **ìºì‹± í™œìš©**: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URLì€ ìŠ¤í‚µ
4. **ì—ëŸ¬ í•¸ë“¤ë§**: ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨ê°€ ì „ì²´ì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡
5. **ë¡œê¹…**: ë””ë²„ê¹…ì„ ìœ„í•œ ì¶©ë¶„í•œ ë¡œê·¸ ì¶œë ¥
